
Visualization of different Deep Learning optimization algorithms on some set of optimization test functions.

Currently implemented algorithms:

1. [SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
2. [Adagrad](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
3. [Adadelta](https://arxiv.org/pdf/1212.5701.pdf)
4. [RMSProp](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
5. [Adam](https://arxiv.org/pdf/1412.6980.pdf)
6. [AdaMax](https://arxiv.org/pdf/1412.6980.pdf)
7. AdamW (WIP)

# Examples

![](assets/beale3d.gif)

![](assets/himmelblau3d.gif)

![](assets/booth3d.gif)

![](assets/beale_2d.png)

![](assets/booth2d.png)

![](assets/himmelblau2d.png)

# REFERENCES

* [Animation code from: Louis Tiao](http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/)
* [An overview of gradient descent optimization algorithms by Sebastian Ruder](https://ruder.io/optimizing-gradient-descent/)